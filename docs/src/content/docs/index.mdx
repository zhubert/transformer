---
title: What is a Transformer?
description: An educational journey through the architecture that powers modern AI
template: splash
hero:
  tagline: Learn how transformers work by building one in PyTorch, from attention mechanisms to complete text generation
  image:
    file: ../../assets/mascot.png
  actions:
    - text: Start Learning
      link: embeddings/
      icon: right-arrow
      variant: primary
    - text: View on GitHub
      link: https://github.com/zhubert/transformer
      icon: external
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Introduction

**What is a transformer?** A transformer is a type of neural network architecture introduced in the landmark paper _"Attention is All You Need"_ (Vaswani et al., 2017). It revolutionized artificial intelligence and is now the foundation of virtually all modern large language models, including GPT, BERT, Claude, and many others.

**What makes transformers special?** Previous approaches to language modeling used recurrent neural networks (RNNs), which process text one word at a time in sequence—like reading a sentence from left to right. Transformers instead use a mechanism called **attention** that allows them to process all words simultaneously _while still understanding their relationships_. This parallel processing makes them much faster to train and more effective at capturing long-range dependencies in text.

## Try It Yourself

<Card title="Get Started Now" icon="rocket" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; color: white;">
  Ready to build your own transformer? Clone the repo and start training in minutes:

  ```bash
  git clone https://github.com/zhubert/transformer.git
  cd transformer
  make install
  python main.py  # Interactive CLI
  ```

  [Full setup guide →](try-it/)
</Card>

## Learning Path

<CardGrid>
  <Card title="Step 1: Token Embeddings" icon="pencil">
    Convert text to vectors and add position information

    [Learn more →](embeddings/)
  </Card>

  <Card title="Step 2: Attention" icon="magnifier">
    Learn how tokens attend to each other using Query, Key, Value

    [Learn more →](attention/)
  </Card>

  <Card title="Step 3: Multi-Head Attention" icon="puzzle">
    Run parallel attention heads to capture different relationships

    [Learn more →](multi-head/)
  </Card>

  <Card title="Step 4: Feed-Forward Networks" icon="setting">
    Process attended information through position-wise MLPs

    [Learn more →](feedforward/)
  </Card>

  <Card title="Step 5: Transformer Block" icon="seti:config">
    Combine attention, FFN, layer norm, and residual connections

    [Learn more →](transformer-block/)
  </Card>

  <Card title="Step 6: Complete Model" icon="rocket">
    Stack blocks and add embedding/output layers

    [Learn more →](complete-model/)
  </Card>

  <Card title="Step 7: Training at Scale" icon="approve-check">
    Use gradient accumulation and validation splits for stable training

    [Learn more →](training/)
  </Card>

  <Card title="Step 8: KV-Cache" icon="forward-slash">
    Optimize inference speed by caching key-value pairs

    [Learn more →](kv-cache/)
  </Card>

  <Card title="Step 9: Interpretability" icon="open-book">
    Analyze attention patterns and understand what the model learns

    [Learn more →](interpretability/)
  </Card>
</CardGrid>
