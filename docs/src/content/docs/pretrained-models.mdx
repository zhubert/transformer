---
title: Pretrained Models
description: Download and use pretrained transformer models
---

import { Aside } from '@astrojs/starlight/components';

## Overview

Instead of training from scratch, you can download pretrained transformer models and use them for:
- **Fine-tuning** on your own datasets
- **Text generation** with a powerful baseline
- **Learning** from production-quality architectures
- **Comparison** with your trained models
- **Interpretability experiments** on sophisticated models

All pretrained models are automatically converted to our checkpoint format, making them compatible with all our training, generation, and interpretability tools.

## Phi-2 (2.7B Parameters)

Microsoft's Phi-2 is a state-of-the-art small language model that punches far above its weight class.

### Quick Start

```bash
# Via interactive CLI (recommended)
python main.py
# ‚Üí Select "ü§ñ Download pretrained models" ‚Üí "phi-2"

# Or directly via command line
python commands/download_phi2.py
```

The model will be saved as: `checkpoints/phi2_pretrained_cl100k.pt`

### Requirements

**Disk Space:**
- Download: ~5.5 GB (model weights in fp32)
- Checkpoint: ~10.8 GB (includes optimizer state for fine-tuning)
- **Total:** ~16 GB free space needed during download

**Memory Requirements:**
- **Generation:** ~6GB GPU memory (or CPU, though 50-100x slower)
- **Fine-tuning:** ~11GB GPU memory with mixed precision (bfloat16)
- **Fine-tuning (fp32):** ~22GB GPU memory
- **Recommendation:** At least 8GB VRAM for comfortable generation, 12GB+ for fine-tuning

**Supported Hardware:**
- ‚úÖ NVIDIA GPUs (CUDA) - Best performance
- ‚úÖ AMD GPUs (ROCm, Linux only) - Excellent performance
- ‚úÖ Apple Silicon (M1/M2/M3 with MPS) - Good performance for generation
- ‚úÖ CPU - Works but very slow (~100x slower than GPU)

**Dependencies:**
- The `transformers` library is automatically included in project dependencies
- First-time download requires internet connection (~5.5 GB)

### Model Specifications

| Specification | Value |
|--------------|-------|
| **Parameters** | 2.7 billion |
| **Layers** | 32 |
| **Model Dimension** | 2560 (d_model) |
| **Attention Heads** | 32 |
| **FFN Dimension** | 10240 (4x expansion) |
| **Vocabulary Size** | 51200 tokens (CodeGen tokenizer) |
| **Position Encoding** | RoPE with partial rotation (40%) |
| **Context Length** | 2048 tokens |
| **Weight Tying** | Yes (embeddings ‚Üî output) |

### What Makes Phi-2 Special?

**Competitive Performance:** Despite being only 2.7B parameters, Phi-2 achieves performance competitive with models 5-10x larger (13B+) on reasoning, coding, and language understanding tasks.

**Modern Architecture:** Uses cutting-edge techniques:
- **Partial RoPE rotation** - Only rotates 40% of dimensions for efficiency
- **Pre-LayerNorm** - Stable training architecture
- **High-quality training data** - Filtered and curated for quality over quantity

**Educational Value:** Perfect size for learning:
- Large enough to show sophisticated behavior
- Small enough to run on consumer hardware
- Modern architecture demonstrates current best practices
- Great baseline for comparing your trained models

**Code-Capable:** Trained with emphasis on coding and technical content, making it excellent for:
- Code generation and completion
- Technical documentation
- Mathematical reasoning
- Logical problem-solving

### Usage After Download

Once downloaded, Phi-2 works with all our tools:

#### Generate Text

```bash
# Via interactive CLI
python main.py
# ‚Üí Select "Generate text" ‚Üí choose phi2_pretrained_cl100k.pt

# Or via command line
python main.py generate checkpoints/phi2_pretrained_cl100k.pt \
  --prompt "Write a Python function to calculate fibonacci numbers" \
  --preset creative \
  --max-length 200
```

#### Fine-tune on Your Data

```bash
# Resume training from Phi-2 checkpoint
python main.py train --resume

# The checkpoint will be automatically detected and training continues
# from the pretrained weights
```

<Aside type="tip" title="Fine-tuning Tips">
When fine-tuning Phi-2:
- Use a **lower learning rate** (1e-5 to 1e-4) than training from scratch
- Start with **small batch sizes** to avoid memory issues
- Enable **mixed precision** (automatic on CUDA) to reduce memory usage
- Monitor **validation loss** carefully to avoid overfitting
</Aside>

#### Evaluate Perplexity

```bash
python main.py evaluate --checkpoint checkpoints/phi2_pretrained_cl100k.pt
```

#### Analyze Internals

```bash
# Logit lens - see how predictions evolve through 32 layers
python main.py interpret logit-lens checkpoints/phi2_pretrained_cl100k.pt --demo

# Attention analysis - discover what Phi-2's heads learned
python main.py interpret attention checkpoints/phi2_pretrained_cl100k.pt --demo

# Induction heads - find pattern-matching circuits
python main.py interpret induction-heads checkpoints/phi2_pretrained_cl100k.pt

# Activation patching - causal experiments
python main.py interpret patch checkpoints/phi2_pretrained_cl100k.pt \
  --clean "The Eiffel Tower is located in" \
  --corrupted "The Statue of Liberty is located in" \
  --target "Paris"
```

### Architecture Deep Dive

#### Partial RoPE Rotation

Phi-2 uses an interesting variant of RoPE (Rotary Position Embeddings) called **partial rotation**:

- **Standard RoPE:** Rotates all dimensions of the query/key vectors
- **Partial RoPE (Phi-2):** Only rotates 40% of dimensions (`partial_rotary_factor=0.4`)

**Why partial rotation?**
- **Performance:** ~10-15% faster attention computation for small batches
- **Capacity:** Preserves more dimensions for pure content representation (not position)
- **Effectiveness:** Still provides excellent positional information

**How it works** (head_dim=80 in Phi-2):
1. First 32 dimensions (40% of 80): Get RoPE applied ‚Üí encode position
2. Remaining 48 dimensions: Pass through unchanged ‚Üí pure content

This is implemented in our `RotaryPositionalEmbedding` class:

```python
rope = RotaryPositionalEmbedding(
    head_dim=80,
    partial_rotary_factor=0.4  # Only rotate 40%
)
```

See [`src/transformer/embeddings.py`](https://github.com/zhubert/transformer/blob/main/src/transformer/embeddings.py) for the full implementation with detailed comments.

#### Weight Conversion

The download script automatically converts Phi-2's weights from HuggingFace format to our architecture:

| Phi-2 Component | Our Architecture |
|----------------|------------------|
| `model.embed_tokens` | `token_embedding.embedding` |
| `model.layers[i].input_layernorm` | `blocks[i].norm1` and `blocks[i].norm2` |
| `model.layers[i].self_attn.{q,k,v}_proj` | `blocks[i].attention.W_{q,k,v}` |
| `model.layers[i].self_attn.dense` | `blocks[i].attention.W_o` |
| `model.layers[i].mlp.fc{1,2}` | `blocks[i].ffn.linear{1,2}` |
| `model.final_layernorm` | `ln_f` |
| `lm_head` | `output_proj` (tied with embeddings) |

### Tokenizer Notes

Phi-2 originally uses the **CodeGen tokenizer** (50,304 tokens), but we convert token IDs to **cl100k_base** (GPT-4 tokenizer, ~100K tokens) for consistency with our training infrastructure.

**What this means:**
- ‚úÖ Generation works seamlessly with our tools
- ‚úÖ You can fine-tune with FineWeb or other cl100k_base datasets
- ‚úÖ Vocabulary size difference is handled automatically
- ‚ö†Ô∏è Very slight differences in tokenization compared to original Phi-2

### Troubleshooting

**Out of Memory during download?**
- Close other applications to free RAM
- The download requires ~6-8GB RAM temporarily

**Out of Memory during generation?**
- Use `--max-length` flag to limit output length
- Try CPU mode: `python main.py generate checkpoints/phi2_pretrained_cl100k.pt --cpu`
- Close other GPU applications

**Out of Memory during fine-tuning?**
- Reduce batch size or increase gradient accumulation steps
- Use mixed precision (automatic on CUDA)
- Consider using a smaller model for fine-tuning experiments

**Download interrupted?**
- Simply run the command again - HuggingFace caching will resume
- Check you have enough disk space (~16 GB free)

## Future Models

We're planning to add support for more pretrained models:
- **LLaMA 2 (7B)** - Meta's open-source model with full RoPE
- **GPT-2 variants** - Historical comparison with learned embeddings
- **Smaller models** - For faster experimentation (350M-1B params)

Want a specific model? [Open an issue on GitHub](https://github.com/zhubert/transformer/issues)!

## Technical Details

### Checkpoint Format

All pretrained models are saved in the same format as our training checkpoints:

```python
{
    'model_state_dict': {...},      # Model weights
    'optimizer_state_dict': {...},  # Placeholder for fine-tuning
    'epoch': 0,                      # Pretrained = epoch 0
    'train_loss': 0.0,
    'val_loss': 0.0,
    'encoding_name': 'cl100k_base',
    'model_config': {
        'num_layers': 32,
        'num_heads': 32,
        'd_model': 2560,
        'd_ff': 10240,
        'vocab_size': 100277,
        'dropout': 0.0,
        'max_seq_len': 2048,
        'tie_weights': True,
        'position_encoding_type': 'rope',
        'partial_rotary_factor': 0.4  # Phi-2 specific
    }
}
```

This unified format means:
- ‚úÖ Load with standard `load_checkpoint()` function
- ‚úÖ Resume training with `--resume` flag
- ‚úÖ Compatible with all generation strategies
- ‚úÖ Works with all interpretability tools

### Performance Benchmarks

Generation speed on various hardware (200 tokens):

| Hardware | Speed | KV-Cache Speedup |
|----------|-------|------------------|
| NVIDIA RTX 4090 | ~50 tokens/sec | 15-20x |
| NVIDIA RTX 3080 | ~35 tokens/sec | 12-18x |
| AMD RX 7900 XTX | ~30 tokens/sec | 10-15x |
| Apple M2 Max (MPS) | ~12 tokens/sec | 5-8x |
| Apple M1 Pro (MPS) | ~8 tokens/sec | 4-6x |
| CPU (12-core) | ~0.5 tokens/sec | 2-3x |

*Note: Actual speeds vary based on prompt length, batch size, and other factors*

---

_Pretrained models are provided for educational purposes. Phi-2 is licensed under the [Microsoft Research License](https://huggingface.co/microsoft/phi-2). Please review the license before using in your projects._
