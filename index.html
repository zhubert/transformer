<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Transformer with AI</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display", "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #2c2c2c;
            background: white;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            text-rendering: optimizeLegibility;
        }

        .container {
            max-width: 1120px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Hero Section */
        .hero {
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            background: linear-gradient(135deg, #ffffff 0%, #f5f5f7 100%);
            padding: 120px 0;
        }

        .hero-content {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 48px;
            text-align: center;
        }

        .hero-image {
            flex-shrink: 0;
        }

        .hero-text {
            flex: 1;
        }

        .hero h1 {
            font-size: clamp(2.5rem, 8vw, 5.5rem);
            font-weight: 700;
            margin-bottom: 24px;
            background: linear-gradient(135deg, #007AFF 0%, #5856D6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            letter-spacing: -0.02em;
        }

        .hero p {
            font-size: clamp(1.125rem, 2vw, 1.5rem);
            color: #6e6e73;
            max-width: 800px;
            margin: 0 auto 32px;
        }

        .hero .subtitle {
            font-size: clamp(1rem, 1.5vw, 1.25rem);
            color: #86868b;
            max-width: 700px;
            margin: 0 auto;
        }

        .hero-mascot {
            max-width: 300px;
            width: 100%;
            height: auto;
            filter: drop-shadow(0 4px 12px rgba(0, 0, 0, 0.1));
        }

        /* Side-by-side layout on wider screens */
        @media (min-width: 1024px) {
            .hero-content {
                flex-direction: row;
                align-items: center;
                gap: 64px;
                text-align: left;
            }

            .hero-text {
                max-width: 600px;
            }

            .hero p,
            .hero .subtitle {
                margin-left: 0;
                margin-right: 0;
            }

            .hero-mascot {
                max-width: 350px;
            }
        }

        /* Section Styles */
        section {
            padding: 120px 0;
        }

        section:nth-child(even) {
            background: linear-gradient(180deg, #ffffff 0%, #f5f5f7 100%);
        }

        h2 {
            font-size: clamp(2rem, 5vw, 3.5rem);
            font-weight: 700;
            margin-bottom: 48px;
            color: #1D1D1F;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: clamp(1.5rem, 3vw, 2rem);
            font-weight: 600;
            margin-bottom: 24px;
            color: #007AFF;
            letter-spacing: -0.005em;
        }

        p {
            font-size: 1.125rem;
            line-height: 1.75;
            color: #333;
            margin-bottom: 1.5em;
            max-width: 70ch;
        }

        /* List Styles */
        ul, ol {
            line-height: 1.8;
            margin-bottom: 32px;
        }

        li {
            margin-bottom: 12px;
        }

        li:last-child {
            margin-bottom: 0;
        }

        /* Card Styles */
        .card {
            background: white;
            border-radius: 24px;
            padding: 48px;
            margin-bottom: 48px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.12);
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-8px);
            box-shadow: 0 16px 48px rgba(0, 0, 0, 0.16);
        }

        /* Code Block Styles */
        .code-window {
            background: #1D1D1F;
            border-radius: 12px;
            overflow: hidden;
            margin: 32px 0;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
        }

        .code-header {
            background: #2D2D2F;
            padding: 12px 16px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .code-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
        }

        .code-dot.red { background: #FF5F57; }
        .code-dot.yellow { background: #FEBC2E; }
        .code-dot.green { background: #28C840; }

        .code-content {
            padding: 24px;
            overflow-x: auto;
        }

        pre {
            margin: 0;
            font-family: "SF Mono", Monaco, Consolas, "Courier New", monospace;
            font-size: 0.9rem;
            line-height: 1.7;
        }

        .code-content code {
            color: #F8F8F2;
            line-height: 1.7;
            font-size: 0.9rem;
        }

        /* Inline code in text */
        p code, li code, .card code:not(.code-content code) {
            color: #007AFF;
            background: #F5F5F7;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: "SF Mono", Monaco, Consolas, "Courier New", monospace;
            font-size: 0.875em;
            font-weight: 500;
        }

        /* Syntax highlighting */
        .keyword { color: #FF79C6; }
        .string { color: #50FA7B; }
        .comment { color: #6272A4; }
        .function { color: #8BE9FD; }
        .class { color: #FFB86C; }

        /* Terminal Styles */
        .terminal {
            background: #1D1D1F;
            border-radius: 12px;
            overflow: hidden;
            margin: 32px 0;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
        }

        .terminal-header {
            background: #2D2D2F;
            padding: 12px 16px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .terminal-content {
            padding: 24px;
            font-family: "SF Mono", Monaco, Consolas, "Courier New", monospace;
            font-size: 1rem;
            line-height: 2;
        }

        .terminal-line {
            color: #F8F8F2;
            margin-bottom: 8px;
        }

        .terminal-prompt {
            color: #50FA7B;
        }

        .terminal-command {
            color: #8BE9FD;
        }

        .terminal-comment {
            color: #6272A4;
        }

        /* Diagram Styles */
        .diagram {
            margin: 48px 0;
            text-align: center;
        }

        .diagram svg {
            max-width: 100%;
            height: auto;
        }

        /* Grid Layout */
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 32px;
            margin: 48px 0;
        }

        .grid-item {
            background: white;
            border-radius: 16px;
            padding: 32px;
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.08);
            transition: all 0.3s ease;
        }

        .grid-item:hover {
            transform: translateY(-4px);
            box-shadow: 0 8px 24px rgba(0, 0, 0, 0.12);
        }

        .grid-item h4 {
            color: #007AFF;
            font-size: 1.25rem;
            margin-bottom: 16px;
        }

        /* Highlight Box */
        .highlight {
            background: linear-gradient(135deg, #007AFF10 0%, #5856D610 100%);
            border-left: 4px solid #007AFF;
            padding: 24px;
            border-radius: 8px;
            margin: 32px 0;
        }

        .highlight p {
            margin-bottom: 0;
            color: #1D1D1F;
            font-size: 1.0625rem;
        }

        /* Focus States */
        .card > p:first-of-type {
            font-size: 1.125rem;
            font-weight: 500;
        }

        /* Responsive */
        @media (max-width: 768px) {
            body {
                font-size: 1rem;
            }

            .hero {
                padding: 80px 0;
            }

            section {
                padding: 80px 0;
            }

            .card {
                padding: 32px 24px;
            }

            p {
                line-height: 1.8;
            }

            .code-content, .terminal-content {
                padding: 16px;
                font-size: 0.875rem;
            }
        }

        /* Footer */
        footer {
            background: #1D1D1F;
            color: #F8F8F2;
            padding: 80px 0 0;
        }

        .footer-content {
            display: grid;
            grid-template-columns: 2fr 1fr 1fr 1fr;
            gap: 60px;
            padding-bottom: 48px;
        }

        .footer-section h4 {
            color: white;
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 16px;
        }

        .footer-section p {
            color: #86868b;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        .footer-links {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .footer-links li {
            margin-bottom: 12px;
        }

        .footer-links a {
            color: #86868b;
            text-decoration: none;
            font-size: 0.875rem;
            transition: color 0.3s ease;
        }

        .footer-links a:hover {
            color: white;
        }

        .footer-bottom {
            border-top: 1px solid #2C2C2E;
            padding: 24px 0;
            text-align: center;
        }

        .footer-bottom p {
            color: #86868b;
            font-size: 0.875rem;
            margin: 0;
        }

        @media (max-width: 1024px) {
            .footer-content {
                grid-template-columns: 1fr 1fr;
                gap: 40px;
            }
        }

        @media (max-width: 640px) {
            .footer-content {
                grid-template-columns: 1fr;
                gap: 32px;
            }
        }

        /* Dark Mode */
        @media (prefers-color-scheme: dark) {
            body {
                color: #f5f5f7;
                background: #000000;
            }

            .hero {
                background: linear-gradient(135deg, #000000 0%, #1c1c1e 100%);
            }

            section:nth-child(even) {
                background: linear-gradient(180deg, #000000 0%, #1c1c1e 100%);
            }

            h2 {
                color: #f5f5f7;
            }

            h3 {
                color: #0a84ff;
            }

            p {
                color: #e5e5e7;
            }

            .hero p {
                color: #98989d;
            }

            .hero .subtitle {
                color: #86868b;
            }

            .card {
                background: #1c1c1e;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6);
            }

            .card:hover {
                box-shadow: 0 16px 48px rgba(0, 0, 0, 0.8);
            }

            p code, li code, .card code:not(.code-content code) {
                color: #0a84ff;
                background: #2c2c2e;
            }

            .code-window {
                background: #1c1c1e;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6);
            }

            .code-header {
                background: #2c2c2e;
            }

            .terminal {
                background: #1c1c1e;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6);
            }

            .terminal-header {
                background: #2c2c2e;
            }

            .component-card {
                background: #1c1c1e;
            }

            .status-badge {
                color: #0a84ff;
            }

            .step-card {
                background: linear-gradient(135deg, rgba(10, 132, 255, 0.1) 0%, rgba(88, 86, 214, 0.1) 100%);
            }

            .step-number {
                color: #e5e5e7;
            }

            footer {
                background: #1c1c1e;
                color: #f5f5f7;
            }

            .footer-bottom {
                border-top: 1px solid #2c2c2e;
            }

            .footer-links a {
                color: #98989d;
            }

            .footer-links a:hover {
                color: #f5f5f7;
            }

            .footer-bottom p {
                color: #98989d;
            }

            .highlight p {
                color: #f5f5f7;
            }

            .grid-item {
                background: #1c1c1e;
                box-shadow: 0 4px 16px rgba(0, 0, 0, 0.6);
            }

            .grid-item:hover {
                box-shadow: 0 8px 24px rgba(0, 0, 0, 0.8);
            }

            .grid-item h4 {
                color: #0a84ff;
            }

            /* SVG dark mode fixes */
            svg rect[fill="#f5f5f7"],
            svg rect[fill="#F5F5F7"] {
                fill: #2c2c2e !important;
            }

            svg text[fill="#1D1D1F"],
            svg text[fill="#1d1d1f"] {
                fill: #f5f5f7 !important;
            }

            svg text[fill="#666"],
            svg text[fill="#666666"] {
                fill: #98989d !important;
            }

            svg line[stroke="#1D1D1F"],
            svg line[stroke="#1d1d1f"] {
                stroke: #f5f5f7 !important;
            }

            /* Inline style overrides for dark mode */
            p[style*="background: #f5f5f7"] {
                background: #2c2c2e !important;
                color: #f5f5f7 !important;
            }

            p[style*="color: #666"] {
                color: #98989d !important;
            }
        }
    </style>
</head>
<body>
    <!-- Hero Section -->
    <div class="hero">
        <div class="container">
            <div class="hero-content">
                <div class="hero-image">
                    <img src="assets/mascot.png" alt="Transformer Mascot" class="hero-mascot">
                </div>
                <div class="hero-text">
                    <h1>Building a Transformer with AI</h1>
                    <p>An educational journey through the architecture that powers modern AI</p>
                    <p class="subtitle">Learn how transformers work by building one in PyTorch, from attention mechanisms to complete text generation</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Introduction -->
    <section>
        <div class="container">
            <h2>What is a Transformer?</h2>
            <div class="card">
                <p><strong>What is a transformer?</strong> A transformer is a type of neural network architecture introduced in the landmark paper <em>"Attention is All You Need"</em> (Vaswani et al., 2017). It revolutionized artificial intelligence and is now the foundation of virtually all modern large language models, including GPT, BERT, Claude, and many others.</p>

                <p><strong>What makes transformers special?</strong> Previous approaches to language modeling used recurrent neural networks (RNNs), which process text one word at a time in sequence—like reading a sentence from left to right. Transformers instead use a mechanism called <strong>attention</strong> that allows them to process all words simultaneously <em>while still understanding their relationships</em>. This parallel processing makes them much faster to train and more effective at capturing long-range dependencies in text.</p>

                <p><strong>A simple analogy:</strong> Imagine you're reading a mystery novel and encounter the sentence "The butler did it." To understand this, you need to recall earlier mentions of the butler throughout the book. An RNN would have to maintain this context through many steps of sequential processing. A transformer can directly "look back" at all previous mentions and decide which ones are relevant—that's attention in action.</p>

                <div class="highlight">
                    <p><strong>Why build incrementally with AI?</strong> You could use a pre-built transformer from a library, but building one yourself provides deep understanding of how these models actually work. This helps you debug issues, make architecture choices, optimize performance, and innovate on the design. Plus, it's surprisingly achievable—the core concepts are elegant and understandable!</p>
                </div>
            </div>

            <h3>Key Components</h3>
            <div class="grid">
                <div class="grid-item">
                    <h4>Self-Attention</h4>
                    <p>Allows the model to weigh the importance of different words when processing each word in context</p>
                </div>
                <div class="grid-item">
                    <h4>Multi-Head Attention</h4>
                    <p>Runs multiple attention mechanisms in parallel, each learning different relationships</p>
                </div>
                <div class="grid-item">
                    <h4>Positional Encoding</h4>
                    <p>Adds positional information since transformers process all tokens simultaneously</p>
                </div>
                <div class="grid-item">
                    <h4>Feed-Forward Networks</h4>
                    <p>Applied to each position independently to process the attended information</p>
                </div>
                <div class="grid-item">
                    <h4>Layer Normalization</h4>
                    <p>Stabilizes training by normalizing activations within each layer</p>
                </div>
                <div class="grid-item">
                    <h4>Residual Connections</h4>
                    <p>Enable gradient flow through deep networks via "skip connections"</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 1: Embeddings -->
    <section id="embeddings">
        <div class="container">
            <h2>Step 1: Token Embeddings & Positional Encoding</h2>
            <div class="card">
                <h3>Token Embeddings</h3>
                <p><strong>What are tokens?</strong> Before we can process text with a neural network, we need to break it into pieces called tokens. A token might be a word ("hello"), a subword ("ing"), or even a single character. For example, the sentence "The cat sat" might be tokenized as ["The", "cat", "sat"], and each token gets assigned a unique number (ID) from a vocabulary—perhaps "The"=5, "cat"=142, "sat"=89.</p>

                <p><strong>Why do we need embeddings?</strong> Computers can't directly understand these token IDs—they're just arbitrary numbers. We need to convert them into meaningful representations that capture semantic relationships. That's where embeddings come in.</p>

                <p><strong>What is an embedding?</strong> An embedding is a learned vector representation (a list of numbers) for each token. Instead of representing "cat" as the ID 142, we represent it as a dense vector like [0.2, -0.5, 0.8, ...] with <code>d_model</code> dimensions (typically 512 or 768). These vectors are learned during training so that similar words end up with similar vectors.</p>

                <p>Think of this as giving each word a unique coordinate in a high-dimensional space. Words with similar meanings (like "cat" and "kitten") end up close together, while unrelated words (like "cat" and "democracy") are far apart.</p>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">class</span> <span class="class">TokenEmbedding</span>(nn.Module):
    <span class="string">"""Convert token indices to dense vectors."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model):
        <span class="keyword">super</span>().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.d_model = d_model

    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># x: (batch, seq_len) - token indices</span>
        <span class="comment"># returns: (batch, seq_len, d_model) - embeddings</span>
        <span class="keyword">return</span> self.embedding(x)</code></pre>
                    </div>
                </div>

                <h3>Positional Encoding</h3>
                <p><strong>Why do we need positional information?</strong> Consider the sentences "The cat ate the mouse" vs "The mouse ate the cat"—same words, completely different meanings! The order matters. Traditional recurrent neural networks (RNNs) process words one at a time in sequence, so they naturally know the order. But transformers process all tokens simultaneously in parallel (which is faster), so they have no inherent notion of position.</p>

                <p><strong>The solution: Positional encodings.</strong> We add positional information to our embeddings by creating special "position vectors" that encode where each token appears in the sequence. For position 0, we create one vector. For position 1, a different vector. And so on.</p>

                <p><strong>Two approaches:</strong> The original transformer paper used fixed sinusoidal patterns (mathematical functions that encode position), but modern models like GPT-2, GPT-3, and BERT use <em>learned</em> positional embeddings—each position gets its own embedding that the model learns during training, just like token embeddings.</p>

                <p>We add (not concatenate) these position embeddings to the token embeddings, so each token now carries information about both <em>what</em> it is and <em>where</em> it is in the sequence.</p>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">class</span> <span class="class">PositionalEncoding</span>(nn.Module):
    <span class="string">"""Add position information to token embeddings."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, max_seq_len=<span class="string">5000</span>):
        <span class="keyword">super</span>().__init__()
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)

    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># x: (batch, seq_len, d_model)</span>
        batch_size, seq_len, d_model = x.shape

        <span class="comment"># Create position indices: [0, 1, 2, ..., seq_len-1]</span>
        positions = torch.arange(seq_len, device=x.device)

        <span class="comment"># Get position embeddings and add to input</span>
        pos_emb = self.pos_embedding(positions)
        <span class="keyword">return</span> x + pos_emb  <span class="comment"># Broadcasting handles batch</span></code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 2: Attention -->
    <section id="attention">
        <div class="container">
            <h2>Step 2: Scaled Dot-Product Attention</h2>
            <div class="card">
                <p><strong>The core innovation of transformers:</strong> Attention is the mechanism that allows each word to "look at" and gather information from all other words in the sentence. This is what makes transformers so powerful.</p>

                <p><strong>A concrete example:</strong> Consider the sentence "The animal didn't cross the street because it was too tired." What does "it" refer to? A human knows "it" refers to "the animal" (not "the street"). Attention allows the model to learn this—when processing "it", the model can attend strongly to "animal" and incorporate that information.</p>

                <p><strong>How does attention work?</strong> The mechanism uses three components for each token, derived from the input embeddings through learned linear transformations:</p>
                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Query (Q):</strong> "What am I looking for?" - represents what the current token wants to know</li>
                    <li><strong>Key (K):</strong> "What do I contain?" - represents what information each token offers</li>
                    <li><strong>Value (V):</strong> "What information do I have?" - the actual content that gets passed along</li>
                </ul>

                <p><strong>The process:</strong> For each token, we compare its Query against all Keys (using dot products) to compute attention scores—how much should we pay attention to each other token? We normalize these scores with softmax to get probabilities (weights that sum to 1), then use these weights to take a weighted average of all Values.</p>

                <p>The formula is elegant: <strong>Attention(Q, K, V) = softmax(Q·Kᵀ / √d_k) · V</strong></p>

                <p>The division by √d_k is a scaling factor that prevents very large dot products in high dimensions, which would cause the softmax to produce near-zero gradients and make training difficult.</p>

                <div class="diagram">
                    <svg viewBox="0 0 600 400" xmlns="http://www.w3.org/2000/svg">
                        <!-- Query, Key, Value inputs -->
                        <rect x="50" y="50" width="120" height="60" fill="#007AFF" opacity="0.2" rx="8"/>
                        <text x="110" y="85" text-anchor="middle" font-family="SF Pro Display" font-size="18" fill="#007AFF">Query (Q)</text>

                        <rect x="240" y="50" width="120" height="60" fill="#5856D6" opacity="0.2" rx="8"/>
                        <text x="300" y="85" text-anchor="middle" font-family="SF Pro Display" font-size="18" fill="#5856D6">Key (K)</text>

                        <rect x="430" y="50" width="120" height="60" fill="#32D74B" opacity="0.2" rx="8"/>
                        <text x="490" y="85" text-anchor="middle" font-family="SF Pro Display" font-size="18" fill="#32D74B">Value (V)</text>

                        <!-- Arrows to MatMul -->
                        <path d="M 150 110 L 150 160" stroke="#007AFF" stroke-width="3" fill="none" marker-end="url(#arrowblue)"/>
                        <path d="M 270 110 L 270 160" stroke="#5856D6" stroke-width="3" fill="none" marker-end="url(#arrowpurple)"/>

                        <!-- MatMul Q·Kᵀ -->
                        <rect x="120" y="160" width="180" height="50" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="210" y="190" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#FF9500">Q · Kᵀ / √d_k</text>

                        <!-- Arrow to Softmax -->
                        <path d="M 210 210 L 210 240" stroke="#FF9500" stroke-width="3" fill="none" marker-end="url(#arroworange)"/>

                        <!-- Softmax -->
                        <rect x="140" y="240" width="140" height="50" fill="#FF2D55" opacity="0.2" rx="8"/>
                        <text x="210" y="270" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#FF2D55">Softmax</text>

                        <!-- Arrows to final MatMul -->
                        <path d="M 250 265 L 350 265 L 350 320" stroke="#FF2D55" stroke-width="3" fill="none" marker-end="url(#arrowred)"/>
                        <path d="M 490 110 L 490 265 L 390 265 L 390 320" stroke="#32D74B" stroke-width="3" fill="none" marker-end="url(#arrowgreen)"/>

                        <!-- Final MatMul -->
                        <rect x="280" y="320" width="180" height="50" fill="#1D1D1F" opacity="0.15" rx="8"/>
                        <text x="370" y="350" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#1D1D1F">Attention · V</text>

                        <!-- Arrow definitions -->
                        <defs>
                            <marker id="arrowblue" markerWidth="10" markerHeight="10" refX="5" refY="3" orient="auto" markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L9,3 z" fill="#007AFF" />
                            </marker>
                            <marker id="arrowpurple" markerWidth="10" markerHeight="10" refX="5" refY="3" orient="auto" markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L9,3 z" fill="#5856D6" />
                            </marker>
                            <marker id="arroworange" markerWidth="10" markerHeight="10" refX="5" refY="3" orient="auto" markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L9,3 z" fill="#FF9500" />
                            </marker>
                            <marker id="arrowred" markerWidth="10" markerHeight="10" refX="5" refY="3" orient="auto" markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L9,3 z" fill="#FF2D55" />
                            </marker>
                            <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="5" refY="3" orient="auto" markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L9,3 z" fill="#32D74B" />
                            </marker>
                        </defs>
                    </svg>
                </div>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(self, query, key, value, mask=None):
    <span class="comment"># Get dimension for scaling</span>
    d_k = query.size(-<span class="string">1</span>)

    <span class="comment"># Compute attention scores: Q·Kᵀ / √d_k</span>
    scores = torch.matmul(query, key.transpose(-<span class="string">2</span>, -<span class="string">1</span>))
    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))

    <span class="comment"># Apply causal mask if provided (prevent looking at future)</span>
    <span class="keyword">if</span> mask <span class="keyword">is not None</span>:
        scores = scores.masked_fill(mask == <span class="string">1</span>, float(<span class="string">'-inf'</span>))

    <span class="comment"># Apply softmax to get attention weights (probabilities)</span>
    attention_weights = torch.softmax(scores, dim=-<span class="string">1</span>)

    <span class="comment"># Apply attention weights to values</span>
    output = torch.matmul(attention_weights, value)

    <span class="keyword">return</span> output, attention_weights</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Causal masking for language models:</strong> In decoder-only transformers (like GPT), we add a mask to prevent tokens from attending to future positions. This is essential for autoregressive generation—when predicting the next word, the model shouldn't "cheat" by looking ahead! The mask sets future attention scores to -∞ before softmax, making those positions receive zero attention weight.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 3: Multi-Head Attention -->
    <section id="multi-head">
        <div class="container">
            <h2>Step 3: Multi-Head Attention</h2>
            <div class="card">
                <p><strong>Why multiple heads?</strong> A single attention mechanism is powerful, but it can only learn one way of relating tokens. Multi-head attention runs several attention mechanisms in parallel (typically 8 or 16), each called a "head." This gives the model multiple "perspectives" to understand relationships between words.</p>

                <p><strong>What do different heads learn?</strong> Through training, different heads naturally specialize in different types of relationships. Research shows that real models develop heads that focus on:</p>

                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Syntactic relationships:</strong> One head might track subject-verb agreement</li>
                    <li><strong>Semantic relationships:</strong> Another head might connect related concepts</li>
                    <li><strong>Long-range dependencies:</strong> A head might link pronouns to their antecedents</li>
                    <li><strong>Local patterns:</strong> Another head might attend to adjacent words in phrases</li>
                </ul>

                <p><strong>How it works:</strong> We split the d_model dimensions across heads. With d_model=512 and 8 heads, each head operates on 64 dimensions (512/8). All heads process the input in parallel, then we concatenate their outputs and apply a final linear transformation.</p>

                <div class="diagram">
                    <svg viewBox="0 0 700 450" xmlns="http://www.w3.org/2000/svg">
                        <!-- Input -->
                        <rect x="250" y="30" width="200" height="50" fill="#007AFF" opacity="0.2" rx="8"/>
                        <text x="350" y="60" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#007AFF">Input (batch, seq, d_model)</text>

                        <!-- Linear Projections -->
                        <path d="M 350 80 L 350 120" stroke="#007AFF" stroke-width="2" marker-end="url(#arrowblue)"/>

                        <rect x="220" y="120" width="260" height="50" fill="#5856D6" opacity="0.2" rx="8"/>
                        <text x="350" y="150" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#5856D6">Linear Projections (Q, K, V)</text>

                        <!-- Split into heads -->
                        <path d="M 280 170 L 150 210" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>
                        <path d="M 320 170 L 280 210" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>
                        <path d="M 380 170 L 420 210" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>
                        <path d="M 420 170 L 550 210" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>

                        <!-- Attention Heads -->
                        <rect x="70" y="210" width="100" height="60" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="120" y="235" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Head 1</text>
                        <text x="120" y="255" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Attention</text>

                        <rect x="220" y="210" width="100" height="60" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="270" y="235" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Head 2</text>
                        <text x="270" y="255" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Attention</text>

                        <rect x="380" y="210" width="100" height="60" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="430" y="235" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Head 3</text>
                        <text x="430" y="255" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Attention</text>

                        <rect x="530" y="210" width="100" height="60" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="580" y="235" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Head N</text>
                        <text x="580" y="255" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Attention</text>

                        <!-- Concatenate -->
                        <path d="M 150 270 L 280 310" stroke="#FF9500" stroke-width="2" marker-end="url(#arroworange)"/>
                        <path d="M 280 270 L 320 310" stroke="#FF9500" stroke-width="2" marker-end="url(#arroworange)"/>
                        <path d="M 420 270 L 380 310" stroke="#FF9500" stroke-width="2" marker-end="url(#arroworange)"/>
                        <path d="M 550 270 L 420 310" stroke="#FF9500" stroke-width="2" marker-end="url(#arroworange)"/>

                        <rect x="250" y="310" width="200" height="50" fill="#32D74B" opacity="0.2" rx="8"/>
                        <text x="350" y="340" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#32D74B">Concatenate Heads</text>

                        <!-- Output projection -->
                        <path d="M 350 360 L 350 390" stroke="#32D74B" stroke-width="2" marker-end="url(#arrowgreen)"/>

                        <rect x="250" y="390" width="200" height="50" fill="#1D1D1F" opacity="0.15" rx="8"/>
                        <text x="350" y="420" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#1D1D1F">Output Projection</text>
                    </svg>
                </div>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=None):
    batch_size, seq_len, d_model = x.shape

    <span class="comment"># 1. Project input to Q, K, V</span>
    Q = self.W_q(x)  <span class="comment"># (batch, seq_len, d_model)</span>
    K = self.W_k(x)
    V = self.W_v(x)

    <span class="comment"># 2. Split into multiple heads</span>
    <span class="comment"># Reshape: (batch, seq_len, num_heads, d_k)</span>
    Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="string">1</span>, <span class="string">2</span>)
    K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="string">1</span>, <span class="string">2</span>)
    V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="string">1</span>, <span class="string">2</span>)

    <span class="comment"># 3. Apply attention to each head (in parallel!)</span>
    output, attn_weights = self.attention(Q, K, V, mask)

    <span class="comment"># 4. Concatenate heads back together</span>
    output = output.transpose(<span class="string">1</span>, <span class="string">2</span>).contiguous()
    output = output.view(batch_size, seq_len, d_model)

    <span class="comment"># 5. Final linear projection</span>
    output = self.W_o(output)

    <span class="keyword">return</span> output</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Why don't heads learn the same thing?</strong> Different random initializations, different learned projections, and the optimization process all encourage diversity. Redundancy doesn't help reduce loss, so heads naturally specialize.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 4: Feed-Forward Networks -->
    <section id="feedforward">
        <div class="container">
            <h2>Step 4: Position-Wise Feed-Forward Networks</h2>
            <div class="card">
                <p><strong>What is a feed-forward network?</strong> After attention gathers information from across the sequence, we need to actually <em>process</em> that information. The feed-forward network (FFN) is a simple two-layer neural network—also called a Multi-Layer Perceptron (MLP)—that transforms each token's representation independently.</p>

                <p><strong>Why do we need it?</strong> Think of the attention layer as "communication" between tokens—gathering relevant context. The FFN is the "computation" step—processing that gathered information to extract useful features and patterns. Without the FFN, the model would only shuffle information around without transforming it.</p>

                <p><strong>The architecture:</strong></p>
                <ol style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Expand:</strong> Project from d_model (e.g., 512) to d_ff (typically 4× larger, e.g., 2048). This expansion gives the model more "capacity" to learn complex patterns.</li>
                    <li><strong>Activate:</strong> Apply GELU activation—a smooth nonlinear function that allows the model to learn non-linear relationships. Without this nonlinearity, stacking layers would be pointless (multiple linear transformations collapse to one).</li>
                    <li><strong>Project back:</strong> Compress back down from d_ff to d_model so the output shape matches the input, allowing us to stack more layers.</li>
                </ol>

                <p><strong>Position-wise:</strong> Crucially, the <em>same</em> FFN (same weights) is applied to every position independently. This is efficient and helps the model learn general transformations that work regardless of position.</p>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">class</span> <span class="class">FeedForward</span>(nn.Module):
    <span class="string">"""Position-wise feed-forward network."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, d_ff, dropout=<span class="string">0.1</span>):
        <span class="keyword">super</span>().__init__()

        <span class="comment"># Expand dimension</span>
        self.linear1 = nn.Linear(d_model, d_ff)

        <span class="comment"># GELU activation (used in GPT-2, GPT-3)</span>
        self.activation = nn.GELU()
        self.dropout1 = nn.Dropout(dropout)

        <span class="comment"># Project back to d_model</span>
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout2 = nn.Dropout(dropout)

    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># x: (batch, seq_len, d_model)</span>
        x = self.linear1(x)        <span class="comment"># → (batch, seq_len, d_ff)</span>
        x = self.activation(x)
        x = self.dropout1(x)
        x = self.linear2(x)        <span class="comment"># → (batch, seq_len, d_model)</span>
        x = self.dropout2(x)
        <span class="keyword">return</span> x</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Division of labor:</strong> Attention answers "What should I pay attention to?" while the FFN answers "Now that I have this information, what should I do with it?"</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 5: Transformer Block -->
    <section>
        <div class="container">
            <h2>Step 5: Transformer Block</h2>
            <div class="card">
                <p><strong>Bringing it all together:</strong> A transformer block combines all our components into one repeatable unit. The full transformer model is just many of these blocks stacked on top of each other (GPT-3 has 96 blocks!).</p>

                <p><strong>What's in a block?</strong> Each block contains four key components:</p>

                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Multi-head attention:</strong> Communication layer—tokens gather information from other tokens</li>
                    <li><strong>Feed-forward network:</strong> Computation layer—each token processes its gathered information</li>
                    <li><strong>Layer normalization:</strong> Stabilizes training by normalizing activations (prevents them from growing too large or small)</li>
                    <li><strong>Residual connections:</strong> "Skip connections" that create gradient highways for training deep networks</li>
                </ul>

                <p><strong>Pre-LN architecture:</strong> We use the Pre-LN (Pre-Layer Normalization) approach used in modern models like GPT-2 and GPT-3. This means we apply layer normalization <em>before</em> each sub-layer (attention or FFN) rather than after. This makes training more stable, especially for very deep networks.</p>

                <div class="diagram">
                    <svg viewBox="0 0 500 600" xmlns="http://www.w3.org/2000/svg">
                        <!-- Input -->
                        <rect x="150" y="30" width="200" height="50" fill="#007AFF" opacity="0.2" rx="8"/>
                        <text x="250" y="60" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#007AFF">Input</text>

                        <!-- First residual block -->
                        <rect x="50" y="100" width="400" height="180" fill="#f5f5f7" stroke="#007AFF" stroke-width="2" stroke-dasharray="5,5" rx="12"/>

                        <!-- Layer Norm 1 -->
                        <path d="M 250 80 L 250 120" stroke="#007AFF" stroke-width="2" marker-end="url(#arrowblue)"/>
                        <rect x="170" y="120" width="160" height="40" fill="#5856D6" opacity="0.2" rx="8"/>
                        <text x="250" y="145" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#5856D6">LayerNorm</text>

                        <!-- Multi-Head Attention -->
                        <path d="M 250 160 L 250 190" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>
                        <rect x="150" y="190" width="200" height="40" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="250" y="215" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Multi-Head Attention</text>

                        <!-- Residual connection 1 -->
                        <path d="M 100 55 L 100 245 L 180 245" stroke="#32D74B" stroke-width="3" fill="none" marker-end="url(#arrowgreen)"/>
                        <text x="75" y="150" font-family="SF Pro Display" font-size="12" fill="#32D74B">+</text>

                        <circle cx="250" cy="245" r="20" fill="#32D74B" opacity="0.2"/>
                        <text x="250" y="252" text-anchor="middle" font-family="SF Pro Display" font-size="20" fill="#32D74B">+</text>

                        <!-- Second residual block -->
                        <rect x="50" y="300" width="400" height="180" fill="#f5f5f7" stroke="#007AFF" stroke-width="2" stroke-dasharray="5,5" rx="12"/>

                        <!-- Layer Norm 2 -->
                        <path d="M 250 265 L 250 320" stroke="#32D74B" stroke-width="2" marker-end="url(#arrowgreen)"/>
                        <rect x="170" y="320" width="160" height="40" fill="#5856D6" opacity="0.2" rx="8"/>
                        <text x="250" y="345" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#5856D6">LayerNorm</text>

                        <!-- Feed-Forward -->
                        <path d="M 250 360 L 250 390" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>
                        <rect x="170" y="390" width="160" height="40" fill="#FF9500" opacity="0.2" rx="8"/>
                        <text x="250" y="415" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#FF9500">Feed-Forward</text>

                        <!-- Residual connection 2 -->
                        <path d="M 400 245 L 400 445 L 320 445" stroke="#32D74B" stroke-width="3" fill="none" marker-end="url(#arrowgreen)"/>
                        <text x="415" y="350" font-family="SF Pro Display" font-size="12" fill="#32D74B">+</text>

                        <circle cx="250" cy="445" r="20" fill="#32D74B" opacity="0.2"/>
                        <text x="250" y="452" text-anchor="middle" font-family="SF Pro Display" font-size="20" fill="#32D74B">+</text>

                        <!-- Output -->
                        <path d="M 250 465 L 250 520" stroke="#32D74B" stroke-width="2" marker-end="url(#arrowgreen)"/>
                        <rect x="150" y="520" width="200" height="50" fill="#1D1D1F" opacity="0.15" rx="8"/>
                        <text x="250" y="550" text-anchor="middle" font-family="SF Pro Display" font-size="16" fill="#1D1D1F">Output</text>
                    </svg>
                </div>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=None):
    <span class="comment"># First sub-layer: Multi-head attention with residual</span>
    residual = x
    x = self.norm1(x)                    <span class="comment"># Pre-LN</span>
    x = self.attention(x, mask=mask)
    x = self.dropout1(x)
    x = x + residual                     <span class="comment"># Residual connection</span>

    <span class="comment"># Second sub-layer: Feed-forward with residual</span>
    residual = x
    x = self.norm2(x)                    <span class="comment"># Pre-LN</span>
    x = self.ffn(x)
    x = self.dropout2(x)
    x = x + residual                     <span class="comment"># Residual connection</span>

    <span class="keyword">return</span> x</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Why residual connections?</strong> They create gradient "highways" that allow gradients to flow directly from the output back to early layers. Without them, deep networks struggle to learn: ∂(x + f(x))/∂x = 1 + ∂f(x)/∂x—the "1" ensures gradients always flow!</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 6: Complete Model -->
    <section>
        <div class="container">
            <h2>Step 6: The Complete Transformer</h2>
            <div class="card">
                <p><strong>The complete picture:</strong> We now assemble all our components into a working decoder-only transformer (GPT-style). This is a complete language model that can be trained to predict the next word in a sequence.</p>

                <p><strong>What is "decoder-only"?</strong> The original transformer paper had both an encoder (for reading input) and decoder (for generating output), used for translation. Modern language models like GPT use only the decoder part, which is simpler and works great for text generation. The key difference is that decoder-only models use causal masking—they can only look at previous tokens, not future ones.</p>

                <p><strong>How data flows through the model:</strong></p>
                <ol style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Token Embedding:</strong> Convert input token IDs (integers) to dense vectors</li>
                    <li><strong>Positional Encoding:</strong> Add position information to tell the model where each token is</li>
                    <li><strong>Transformer Blocks (×N):</strong> Stack multiple identical blocks (we use 6; GPT-3 uses 96). Each block refines the representations through attention and feed-forward processing</li>
                    <li><strong>Final LayerNorm:</strong> One last normalization to stabilize the final outputs</li>
                    <li><strong>Output Projection:</strong> Project from d_model dimensions to vocabulary size, giving us scores (logits) for every possible next token</li>
                </ol>

                <p><strong>What are logits?</strong> The model outputs "logits"—raw, unnormalized scores for each token in the vocabulary. Higher scores mean the model thinks that token is more likely to come next. We can convert these to probabilities using softmax, then either pick the highest (greedy decoding) or sample from the distribution (for more creative generation).</p>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">def</span> <span class="function">__init__</span>(
    self, vocab_size, d_model=<span class="string">512</span>, num_heads=<span class="string">8</span>,
    num_layers=<span class="string">6</span>, d_ff=<span class="string">2048</span>, max_seq_len=<span class="string">5000</span>, dropout=<span class="string">0.1</span>
):
    <span class="keyword">super</span>().__init__()

    <span class="comment"># Token and positional embeddings</span>
    self.token_embedding = TokenEmbedding(vocab_size, d_model)
    self.pos_encoding = PositionalEncoding(d_model, max_seq_len)

    <span class="comment"># Stack of transformer blocks</span>
    self.blocks = nn.ModuleList([
        TransformerBlock(d_model, num_heads, d_ff, dropout)
        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)
    ])

    <span class="comment"># Final layer norm and output projection</span>
    self.ln_f = nn.LayerNorm(d_model)
    self.output_proj = nn.Linear(d_model, vocab_size)</code></pre>
                    </div>
                </div>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=None):
    <span class="comment"># Create causal mask if not provided</span>
    <span class="keyword">if</span> mask <span class="keyword">is</span> None:
        mask = self.create_causal_mask(x.size(<span class="string">1</span>)).to(x.device)

    <span class="comment"># 1. Embed tokens and add positions</span>
    x = self.token_embedding(x)      <span class="comment"># (batch, seq) → (batch, seq, d_model)</span>
    x = self.pos_encoding(x)

    <span class="comment"># 2. Pass through all transformer blocks</span>
    <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:
        x = block(x, mask=mask)      <span class="comment"># (batch, seq, d_model) → (batch, seq, d_model)</span>

    <span class="comment"># 3. Final normalization and projection to vocabulary</span>
    x = self.ln_f(x)
    logits = self.output_proj(x)     <span class="comment"># (batch, seq, d_model) → (batch, seq, vocab_size)</span>

    <span class="keyword">return</span> logits</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Training the model:</strong> During training, we feed the model sequences of text and ask it to predict the next token at each position. We compare its predictions (logits) against the actual next tokens using cross-entropy loss, then use backpropagation to adjust all the weights (embeddings, attention projections, FFN weights, etc.). After training on billions of tokens, the model learns to predict plausible next words based on context.</p>
                </div>

                <div class="highlight">
                    <p><strong>Model scale:</strong> Our implementation uses 6 layers with d_model=512, similar to the original transformer paper. For comparison, GPT-3 has 96 layers with d_model=12,288. The architecture scales beautifully—the same fundamental components work at wildly different scales!</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Step 7: Training at Scale -->
    <section id="training">
        <div class="container">
            <h2>Step 7: Training at Scale</h2>
            <div class="card">
                <p>Building the transformer architecture is only half the battle. To train it effectively, we need techniques that make training stable, prevent over fitting, and work within the constraints of hobby-scale hardware. This section covers two critical techniques: <strong>gradient accumulation</strong> and <strong>validation splits</strong>.</p>

                <h3>The Challenge: Small Batches, Noisy Training</h3>
                <p><strong>What is a batch?</strong> During training, we process multiple examples together in a "batch." The model makes predictions for all examples, we compute the average loss, then we calculate gradients and update weights. Larger batches give us more stable gradient estimates because we're averaging over more examples.</p>

                <p><strong>The problem with small batches:</strong> On hobby hardware (like an M1 Mac or consumer GPU), we're limited to small batches—typically just 8 sequences at a time. Small batches lead to <em>noisy gradients</em>: each batch gives a slightly different signal about which direction to update the weights, causing erratic training.</p>

                <div class="highlight">
                    <p><strong>Memory bottleneck:</strong> Why can't we just use bigger batches? Each example in a batch requires storing activations in memory for the backward pass. M1 Macs have ~8GB unified memory, and a batch of 8 sequences already uses ~4GB. Doubling to 16 would run out of memory!</p>
                </div>

                <h3>Gradient Accumulation: Large Batches Without the Memory Cost</h3>
                <p><strong>The key insight:</strong> We don't need to process all examples simultaneously! Gradient accumulation lets us simulate large batch sizes by accumulating gradients over multiple small batches before updating weights.</p>

                <p><strong>How it works:</strong></p>
                <ol style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Process batch 1:</strong> Forward pass → Loss → Backward pass → Store gradients (don't update yet!)</li>
                    <li><strong>Process batch 2:</strong> Forward pass → Loss → Backward pass → <em>Add</em> gradients to stored ones</li>
                    <li><strong>Repeat</strong> for N batches (e.g., 16 times)</li>
                    <li><strong>Update weights:</strong> Use the accumulated (averaged) gradients</li>
                </ol>

                <div class="diagram">
                    <svg viewBox="0 0 700 500" xmlns="http://www.w3.org/2000/svg">
                        <!-- Title -->
                        <text x="350" y="30" text-anchor="middle" font-family="SF Pro Display" font-size="20" font-weight="600" fill="#1D1D1F">Gradient Accumulation</text>

                        <!-- Left side: Without Accumulation -->
                        <text x="150" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="16" font-weight="600" fill="#007AFF">Without Accumulation</text>

                        <!-- Batch 1 -->
                        <rect x="50" y="90" width="200" height="60" fill="#FF9500" opacity="0.2" rx="8" stroke="#FF9500" stroke-width="2"/>
                        <text x="150" y="115" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#1D1D1F">Batch 1 (8 sequences)</text>
                        <text x="150" y="135" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Forward → Backward</text>

                        <!-- Arrow -->
                        <path d="M 150 150 L 150 170" stroke="#FF9500" stroke-width="2" marker-end="url(#arroworange)"/>

                        <!-- Update -->
                        <circle cx="150" cy="185" r="25" fill="#32D74B" opacity="0.3" stroke="#32D74B" stroke-width="2"/>
                        <text x="150" y="192" text-anchor="middle" font-family="SF Pro Display" font-size="14" font-weight="600" fill="#32D74B">Update</text>

                        <!-- Noisy label -->
                        <text x="150" y="230" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#FF2D55">Noisy gradients!</text>

                        <!-- Right side: With Accumulation -->
                        <text x="525" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="16" font-weight="600" fill="#5856D6">With Accumulation (16×)</text>

                        <!-- Multiple batches stacked -->
                        <rect x="425" y="90" width="200" height="40" fill="#FF9500" opacity="0.2" rx="8" stroke="#FF9500" stroke-width="2"/>
                        <text x="525" y="115" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Batch 1 → accumulate</text>

                        <rect x="425" y="135" width="200" height="40" fill="#FF9500" opacity="0.2" rx="8" stroke="#FF9500" stroke-width="2"/>
                        <text x="525" y="160" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Batch 2 → accumulate</text>

                        <text x="525" y="195" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#666">⋮</text>

                        <rect x="425" y="210" width="200" height="40" fill="#FF9500" opacity="0.2" rx="8" stroke="#FF9500" stroke-width="2"/>
                        <text x="525" y="235" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Batch 16 → accumulate</text>

                        <!-- Arrow -->
                        <path d="M 525 250 L 525 270" stroke="#5856D6" stroke-width="2" marker-end="url(#arrowpurple)"/>

                        <!-- Update -->
                        <circle cx="525" cy="285" r="25" fill="#32D74B" opacity="0.3" stroke="#32D74B" stroke-width="2"/>
                        <text x="525" y="292" text-anchor="middle" font-family="SF Pro Display" font-size="14" font-weight="600" fill="#32D74B">Update</text>

                        <!-- Stable label -->
                        <text x="525" y="330" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#32D74B">Stable gradients!</text>

                        <!-- Bottom comparison -->
                        <rect x="50" y="370" width="600" height="100" fill="#f5f5f7" rx="8"/>
                        <text x="350" y="395" text-anchor="middle" font-family="SF Pro Display" font-size="14" font-weight="600" fill="#1D1D1F">The Mathematics</text>
                        <text x="350" y="420" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Left: gradient from 8 sequences (1,024 tokens)</text>
                        <text x="350" y="440" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">Right: averaged gradient from 128 sequences (16,384 tokens)</text>
                        <text x="350" y="460" text-anchor="middle" font-family="SF Pro Display" font-size="12" font-weight="600" fill="#007AFF">16× more stable, same memory!</text>
                    </svg>
                </div>

                <p><strong>Why this works mathematically:</strong> Gradients are linear, so averaging gradients from N separate batches gives the same result as computing the gradient on one large batch containing all N×batch_size examples. The key formula:</p>

                <p style="text-align: center; font-family: 'SF Mono', monospace; background: #f5f5f7; padding: 16px; border-radius: 8px; margin: 24px 0;">
                    ∇(L₁ + L₂ + ... + Lₙ) = ∇L₁ + ∇L₂ + ... + ∇Lₙ
                </p>

                <p>By accumulating gradients over 16 batches of 8 sequences each, we get gradients equivalent to a batch of 128 sequences—16× more stable!—while only ever holding 8 sequences in memory at once.</p>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="comment"># Without accumulation (noisy)</span>
<span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:
    loss = compute_loss(batch)
    loss.backward()           <span class="comment"># Compute gradients</span>
    optimizer.step()          <span class="comment"># Update every batch (noisy!)</span>
    optimizer.zero_grad()

<span class="comment"># With accumulation (stable)</span>
accumulation_steps = <span class="string">16</span>
<span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):
    loss = compute_loss(batch)
    loss = loss / accumulation_steps  <span class="comment"># Scale for correct averaging</span>
    loss.backward()                   <span class="comment"># Accumulate gradients</span>

    <span class="keyword">if</span> (i + <span class="string">1</span>) % accumulation_steps == <span class="string">0</span>:
        optimizer.step()              <span class="comment"># Update every 16 batches (stable!)</span>
        optimizer.zero_grad()</code></pre>
                    </div>
                </div>

                <h3>Validation: Detecting Overfitting</h3>
                <p><strong>The problem: Memorization vs. Learning</strong></p>
                <p>Imagine a student preparing for an exam. They could:</p>
                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>Memorize answers</strong> to practice problems → Fails on new problems (overfitting)</li>
                    <li><strong>Learn concepts</strong> from practice problems → Succeeds on new problems (good generalization)</li>
                </ul>

                <p>The same happens with neural networks. As training progresses, the model might start memorizing the training data instead of learning general patterns. This is called <strong>overfitting</strong>.</p>

                <p><strong>The solution: Validation split</strong></p>
                <p>We set aside 10% of our data that the model <em>never</em> sees during training. After each epoch, we evaluate the model on this "validation" data. If the model is truly learning patterns (not memorizing), it should perform well on both training and validation data.</p>

                <div class="diagram">
                    <svg viewBox="0 0 700 450" xmlns="http://www.w3.org/2000/svg">
                        <!-- Title -->
                        <text x="350" y="30" text-anchor="middle" font-family="SF Pro Display" font-size="20" font-weight="600" fill="#1D1D1F">Training vs Validation Loss</text>

                        <!-- Axes -->
                        <line x1="80" y1="380" x2="650" y2="380" stroke="#1D1D1F" stroke-width="2"/>
                        <line x1="80" y1="80" x2="80" y2="380" stroke="#1D1D1F" stroke-width="2"/>

                        <!-- X-axis label -->
                        <text x="365" y="410" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#666">Training Epochs →</text>

                        <!-- Y-axis label -->
                        <text x="40" y="230" text-anchor="middle" font-family="SF Pro Display" font-size="14" fill="#666" transform="rotate(-90 40 230)">Loss →</text>

                        <!-- Scenario 1: Good training (both decreasing) -->
                        <text x="200" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="13" font-weight="600" fill="#32D74B">✓ Good: Both Improving</text>

                        <!-- Training curve (good) -->
                        <path d="M 100 150 Q 150 130 200 120" stroke="#007AFF" stroke-width="3" fill="none"/>
                        <circle cx="200" cy="120" r="4" fill="#007AFF"/>

                        <!-- Validation curve (good) -->
                        <path d="M 100 160 Q 150 142 200 133" stroke="#5856D6" stroke-width="3" stroke-dasharray="5,5" fill="none"/>
                        <circle cx="200" cy="133" r="4" fill="#5856D6"/>

                        <!-- Scenario 2: Overfitting (val increases) -->
                        <text x="450" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="13" font-weight="600" fill="#FF2D55">⚠ Overfitting: Val Worsening</text>

                        <!-- Training curve (overfitting) -->
                        <path d="M 350 150 Q 400 130 450 100 Q 500 80 550 70" stroke="#007AFF" stroke-width="3" fill="none"/>
                        <circle cx="550" cy="70" r="4" fill="#007AFF"/>

                        <!-- Validation curve (overfitting) -->
                        <path d="M 350 160 Q 400 142 450 135 Q 500 145 550 170" stroke="#5856D6" stroke-width="3" stroke-dasharray="5,5" fill="none"/>
                        <circle cx="550" cy="170" r="4" fill="#5856D6"/>

                        <!-- Divergence annotation -->
                        <path d="M 480 110 L 480 150" stroke="#FF2D55" stroke-width="2" marker-start="url(#arrowred)" marker-end="url(#arrowred)"/>
                        <text x="500" y="135" font-family="SF Mono" font-size="11" fill="#FF2D55">Diverging!</text>

                        <!-- Legend -->
                        <rect x="150" y="390" width="400" height="50" fill="#f5f5f7" rx="8"/>
                        <line x1="170" y1="410" x2="210" y2="410" stroke="#007AFF" stroke-width="3"/>
                        <text x="220" y="415" font-family="SF Pro Display" font-size="12" fill="#1D1D1F">Train Loss (seen data)</text>

                        <line x1="370" y1="410" x2="410" y2="410" stroke="#5856D6" stroke-width="3" stroke-dasharray="5,5"/>
                        <text x="420" y="415" font-family="SF Pro Display" font-size="12" fill="#1D1D1F">Val Loss (unseen data)</text>
                    </svg>
                </div>

                <p><strong>How to interpret the curves:</strong></p>
                <div class="grid">
                    <div class="grid-item">
                        <h4 style="color: #32D74B;">✓ Good Training</h4>
                        <p style="font-family: 'SF Mono', monospace; font-size: 0.9em; color: #666;">
                            Train: 5.0 → 4.0 → 3.0<br/>
                            Val:   5.2 → 4.2 → 3.2
                        </p>
                        <p>Both losses decreasing together. Model is learning general patterns that work on new data!</p>
                    </div>
                    <div class="grid-item">
                        <h4 style="color: #FF9500;">⚠ Underfitting</h4>
                        <p style="font-family: 'SF Mono', monospace; font-size: 0.9em; color: #666;">
                            Train: 5.0 → 4.8 → 4.7<br/>
                            Val:   5.2 → 5.0 → 4.9
                        </p>
                        <p>Both losses barely improving. Model is too simple or needs more training epochs.</p>
                    </div>
                    <div class="grid-item">
                        <h4 style="color: #FF2D55;">⚠ Overfitting</h4>
                        <p style="font-family: 'SF Mono', monospace; font-size: 0.9em; color: #666;">
                            Train: 5.0 → 3.0 → 1.5<br/>
                            Val:   5.2 → 3.5 → 4.0
                        </p>
                        <p>Training loss decreasing but validation increasing. Model is memorizing training data!</p>
                    </div>
                </div>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="comment"># Training with validation</span>
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):
    <span class="comment"># Training phase</span>
    model.train()
    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:
        <span class="comment"># ... forward, backward, update ...</span>

    <span class="comment"># Validation phase (no weight updates!)</span>
    model.eval()
    <span class="keyword">with</span> torch.no_grad():
        <span class="keyword">for</span> batch <span class="keyword">in</span> val_dataloader:
            val_loss = compute_loss(batch)
            <span class="comment"># Just measure, don't update</span>

    print(<span class="string">f"Train loss: {train_loss:.2f}, Val loss: {val_loss:.2f}"</span>)

    <span class="comment"># Check for overfitting</span>
    <span class="keyword">if</span> val_loss > train_loss * <span class="string">1.3</span>:
        print(<span class="string">"Warning: Possible overfitting!"</span>)</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Implementation in this project:</strong> Our training script automatically splits FineWeb into 90% training and 10% validation using a deterministic hash-based split. After each epoch, you'll see both training and validation metrics, along with interpretation hints to help you understand if your model is learning well!</p>
                </div>

                <h3>Expected Improvements</h3>
                <p>With gradient accumulation and validation:</p>
                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>20-30% lower final loss</strong> due to stable training</li>
                    <li><strong>Smoother training curves</strong> that are easier to debug</li>
                    <li><strong>Confidence in generalization</strong> by monitoring validation</li>
                    <li><strong>Early stopping</strong> when validation stops improving</li>
                    <li><strong>Works on hobby hardware</strong> without expensive GPUs</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Step 8: KV-Cache -->
    <section id="kv-cache">
        <div class="container">
            <h2>Step 8: Fast Generation with KV-Cache</h2>
            <div class="card">
                <p><strong>The Problem: Slow Autoregressive Generation</strong></p>
                <p>When generating text, transformers produce one token at a time. After generating each token, we feed the entire sequence back through the model to predict the next token. This means we repeatedly recompute the same values!</p>

                <p><strong>Example without cache:</strong></p>
                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="comment"># Generate "The cat sat"</span>
<span class="comment"># Step 1: Generate token 3</span>
Input: [The, cat]
Compute: K[The], V[The], K[cat], V[cat]
Output: "sat" ✓

<span class="comment"># Step 2: Generate token 4</span>
Input: [The, cat, sat]
Compute: K[The], V[The], K[cat], V[cat], K[sat], V[sat]  <span class="comment">← Redundant!</span>
Output: "on"

<span class="comment"># Step 3: Generate token 5</span>
Input: [The, cat, sat, on]
Compute: K[The], V[The], K[cat], V[cat], K[sat], V[sat], K[on], V[on]  <span class="comment">← Redundant!</span>
Output: "the"</code></pre>
                    </div>
                </div>

                <p>For generating n tokens, we process 1 + 2 + 3 + ... + n = O(n²) tokens total. Very slow!</p>

                <h3>The Solution: KV-Cache</h3>
                <p><strong>Key Insight:</strong> In attention, K (Key) and V (Value) for past tokens never change! Only the new token's query matters. We can cache K and V from previous steps and reuse them.</p>

                <div class="diagram">
                    <svg viewBox="0 0 700 500" xmlns="http://www.w3.org/2000/svg">
                        <!-- Title -->
                        <text x="350" y="30" text-anchor="middle" font-family="SF Pro Display" font-size="20" font-weight="600" fill="#1D1D1F">KV-Cache: Reuse Past Computations</text>

                        <!-- Without Cache (left side) -->
                        <text x="150" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="16" font-weight="600" fill="#FF2D55">Without Cache: O(n²)</text>

                        <!-- Step boxes -->
                        <rect x="50" y="90" width="200" height="60" fill="#FF2D55" opacity="0.2" rx="8" stroke="#FF2D55" stroke-width="2"/>
                        <text x="150" y="115" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 1: Process [tok1, tok2]</text>
                        <text x="150" y="135" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute K,V for 2 tokens</text>

                        <rect x="50" y="160" width="200" height="60" fill="#FF2D55" opacity="0.2" rx="8" stroke="#FF2D55" stroke-width="2"/>
                        <text x="150" y="185" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 2: Process [tok1, tok2, tok3]</text>
                        <text x="150" y="205" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute K,V for 3 tokens ❌</text>

                        <rect x="50" y="230" width="200" height="60" fill="#FF2D55" opacity="0.2" rx="8" stroke="#FF2D55" stroke-width="2"/>
                        <text x="150" y="255" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 3: Process all 4 tokens</text>
                        <text x="150" y="275" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute K,V for 4 tokens ❌</text>

                        <text x="150" y="310" text-anchor="middle" font-family="SF Pro Display" font-size="13" fill="#FF2D55">Total: 2+3+4 = 9 token computations</text>

                        <!-- With Cache (right side) -->
                        <text x="525" y="70" text-anchor="middle" font-family="SF Pro Display" font-size="16" font-weight="600" fill="#32D74B">With Cache: O(n)</text>

                        <rect x="425" y="90" width="200" height="60" fill="#32D74B" opacity="0.2" rx="8" stroke="#32D74B" stroke-width="2"/>
                        <text x="525" y="115" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 1: Process [tok1, tok2]</text>
                        <text x="525" y="135" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute & cache K,V for 2</text>

                        <rect x="425" y="160" width="200" height="60" fill="#32D74B" opacity="0.2" rx="8" stroke="#32D74B" stroke-width="2"/>
                        <text x="525" y="185" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 2: Process [tok3] only!</text>
                        <text x="525" y="205" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute K,V for 1, reuse cache ✓</text>

                        <rect x="425" y="230" width="200" height="60" fill="#32D74B" opacity="0.2" rx="8" stroke="#32D74B" stroke-width="2"/>
                        <text x="525" y="255" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#1D1D1F">Step 3: Process [tok4] only!</text>
                        <text x="525" y="275" text-anchor="middle" font-family="SF Mono" font-size="11" fill="#666">Compute K,V for 1, reuse cache ✓</text>

                        <text x="525" y="310" text-anchor="middle" font-family="SF Pro Display" font-size="13" fill="#32D74B">Total: 2+1+1 = 4 token computations</text>

                        <!-- Speedup annotation -->
                        <rect x="200" y="340" width="300" height="100" fill="#007AFF" opacity="0.1" rx="8"/>
                        <text x="350" y="370" text-anchor="middle" font-family="SF Pro Display" font-size="18" font-weight="600" fill="#007AFF">2.25x Speedup!</text>
                        <text x="350" y="395" text-anchor="middle" font-family="SF Mono" font-size="13" fill="#666">9 computations → 4 computations</text>
                        <text x="350" y="420" text-anchor="middle" font-family="SF Mono" font-size="12" fill="#666">For 200 tokens: ~50x speedup!</text>
                    </svg>
                </div>

                <h3>How It Works</h3>
                <p><strong>Two Modes:</strong></p>
                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li><strong>PREFILL:</strong> Process initial prompt, compute and cache K, V for all tokens</li>
                    <li><strong>DECODE:</strong> For each new token, compute only its K, V, concatenate with cached values</li>
                </ul>

                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="comment"># PREFILL: Process prompt "The cat"</span>
prompt = [The, cat]
K_all = [K_The, K_cat]  <span class="comment"># Cache these!</span>
V_all = [V_The, V_cat]  <span class="comment"># Cache these!</span>
Output: "sat"

<span class="comment"># DECODE: Generate next token</span>
new_token = [sat]
K_new = [K_sat]  <span class="comment"># Only compute for new token</span>
V_new = [V_sat]
K_all = concat(K_cached, K_new)  <span class="comment"># = [K_The, K_cat, K_sat]</span>
V_all = concat(V_cached, V_new)  <span class="comment"># = [V_The, V_cat, V_sat]</span>
Output: "on"

<span class="comment"># Continue...</span></code></pre>
                    </div>
                </div>

                <h3>Memory vs Speed Tradeoff</h3>
                <p><strong>Memory Cost:</strong> For each layer, we cache K and V tensors with shape (batch, num_heads, seq_len, d_k). For a 6-layer model with d_model=256, 4 heads, and 200-token sequence, this is only ~3 MB per example. Very affordable!</p>

                <p><strong>Speed Benefit:</strong> Reduces time complexity from O(n²) to O(n) for generating n tokens. Typical speedups:</p>
                <ul style="margin-left: 24px; margin-bottom: 24px;">
                    <li>Short sequences (10-20 tokens): 2-5x faster</li>
                    <li>Medium sequences (50-100 tokens): 10-20x faster</li>
                    <li>Long sequences (200+ tokens): 20-50x faster</li>
                </ul>

                <div class="highlight">
                    <p><strong>Why ALL production LLMs use KV-cache:</strong> The memory cost is tiny compared to the model weights, but the speed improvement is massive. Every production system (GPT, Claude, etc.) uses KV-cache for generation!</p>
                </div>

                <h3>Using KV-Cache</h3>
                <div class="code-window">
                    <div class="code-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="code-content">
                        <pre><code><span class="comment"># KV-cache is enabled by default!</span>
generated = model.generate(
    start_tokens,
    max_length=<span class="string">100</span>,
    sampling_strategy=<span class="string">"greedy"</span>,
    use_cache=<span class="keyword">True</span>  <span class="comment"># ← Default!</span>
)

<span class="comment"># Disable cache (for debugging/comparison)</span>
generated = model.generate(
    start_tokens,
    max_length=<span class="string">100</span>,
    use_cache=<span class="keyword">False</span>  <span class="comment"># ← Much slower!</span>
)

<span class="comment"># Benchmark the speedup yourself</span>
<span class="keyword">uv run python</span> commands/benchmark_generation.py</code></pre>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Implementation Detail:</strong> The cache must correctly handle positional encodings! When processing token at position N, it must receive position embedding for N, not 0. Our implementation tracks the cache length and adjusts positions automatically.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Try It Yourself -->
    <section>
        <div class="container">
            <h2>Try It Yourself</h2>
            <div class="card">
                <p>Ready to train and experiment with your own transformer? The complete implementation is available on GitHub with everything you need to get started.</p>

                <h3>Quick Start</h3>

                <div class="terminal">
                    <div class="terminal-header">
                        <div class="code-dot red"></div>
                        <div class="code-dot yellow"></div>
                        <div class="code-dot green"></div>
                    </div>
                    <div class="terminal-content">
                        <div class="terminal-line">
                            <span class="terminal-prompt">$</span>
                            <span class="terminal-command"> git clone https://github.com/zhubert/transformer.git</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-prompt">$</span>
                            <span class="terminal-command"> cd transformer</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-comment"># Install dependencies with uv</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-prompt">$</span>
                            <span class="terminal-command"> uv sync</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-comment"># Train the model (uses FineWeb dataset)</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-prompt">$</span>
                            <span class="terminal-command"> uv run python commands/train.py</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-comment"># Generate text</span>
                        </div>
                        <div class="terminal-line">
                            <span class="terminal-prompt">$</span>
                            <span class="terminal-command"> uv run python commands/generate.py --prompt "Once upon a time"</span>
                        </div>
                    </div>
                </div>

                <h3>What You'll Find</h3>
                <div class="grid">
                    <div class="grid-item">
                        <h4>Clean Implementation</h4>
                        <p>Well-documented PyTorch code for every component, incrementally built with AI without using pre-built transformer modules</p>
                    </div>
                    <div class="grid-item">
                        <h4>Real Training</h4>
                        <p>Train on FineWeb, a 10 billion token dataset from HuggingFace with streaming support and smart caching</p>
                    </div>
                    <div class="grid-item">
                        <h4>Multiple Sampling</h4>
                        <p>Experiment with greedy, top-k, top-p, and combined sampling strategies for text generation</p>
                    </div>
                    <div class="grid-item">
                        <h4>Comprehensive Tests</h4>
                        <p>Unit tests for all components to verify your understanding and catch bugs</p>
                    </div>
                </div>

                <div class="highlight">
                    <p><strong>Learning tip:</strong> Start by reading the code in order: embeddings → attention → feedforward → block → model. Each builds on the previous component!</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>Transformer - Incrementally Built with AI</h4>
                    <p>An educational implementation of a decoder-only transformer in PyTorch. Incrementally built with AI to understand the architecture that powers modern AI. This page was generated with assistance from Claude AI and may contain errors.</p>
                </div>

                <div class="footer-section">
                    <h4>Learn More</h4>
                    <ul class="footer-links">
                        <li><a href="https://github.com/zhubert/transformer">View Source Code</a></li>
                        <li><a href="https://github.com/zhubert/transformer/blob/main/README.md">Documentation</a></li>
                        <li><a href="https://github.com/zhubert/transformer/blob/main/CLAUDE.md">Implementation Guide</a></li>
                        <li><a href="https://github.com/zhubert/transformer/tree/main/tests">Tests & Examples</a></li>
                    </ul>
                </div>

                <div class="footer-section">
                    <h4>Components</h4>
                    <ul class="footer-links">
                        <li><a href="#embeddings">Token Embeddings</a></li>
                        <li><a href="#attention">Attention Mechanism</a></li>
                        <li><a href="#multi-head">Multi-Head Attention</a></li>
                        <li><a href="#feedforward">Feed-Forward Network</a></li>
                    </ul>
                </div>

                <div class="footer-section">
                    <h4>Resources</h4>
                    <ul class="footer-links">
                        <li><a href="https://arxiv.org/abs/1706.03762" target="_blank">Original Paper</a></li>
                        <li><a href="http://zhubert.com">More Projects</a></li>
                        <li><a href="https://github.com/zhubert/transformer/issues">Report Issues</a></li>
                        <li><a href="https://pytorch.org" target="_blank">PyTorch Docs</a></li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="footer-bottom">
            <div class="container">
                <p>Built with curiosity and PyTorch. Open source project for educational purposes.</p>
            </div>
        </div>
    </footer>
</body>
</html>
